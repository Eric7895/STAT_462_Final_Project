---
title: "STAT 462 Final Project"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Load necessary Libraries
#library(tidyverse)
#library(ggplot2)
#library(janitor)
#library(kableExtra)
#library(glmnet)

# Load Dataset 
data <- read.csv("insurance.csv")
```

# Group Memeber

Eric Wu, Derek Avery, Allison Schaedler, Xinyi Bao

# Background

The Insurance dataset extracted from Kaggle.com describes the medical costs for over 1300 individuals based on their age, sex, BMI in the United States. The goal of the project is to examine how many children, Smoker or not, and which region in the U.S they lived. The methods of the project are conduct hypothesis tests to determine the difference between sample data that are caused by the sampling error, and compare multiple ML method including multiple linear regression in order to find the best model with the lowest MSE value. In addition, there will be a detail diagnostic on the linear regression model for the assumptions, outliers, and other possible issues. 

# Exploratory Data Analysis
```{r}
# Check for Null Value
na <- sum(is.na(data))

# Create dummy variable
dummy_data <- data %>%
  mutate(
    is_male = if_else(sex == "male", 1, 0),
    is_smoker = if_else(smoker == "yes", 1, 0),
    is_southwest = if_else(region == "southwest", 1, 0),
    is_southeast = if_else(region == "southeast", 1, 0),
    is_northwest = if_else(region == "northwest", 1, 0),
    is_northeast = if_else(region == "northeast", 1, 0)
  ) %>%
  select(-sex, -smoker, -region)

# Create standardized dataset for ML that requires scaling
xvars <- names(dummy_data)
scaled_data <- dummy_data
scaled_data[ , xvars] <- scale(scaled_data[ , xvars],
                            center = TRUE,
                            scale = TRUE)
```

## Data Statistics
```{r}
# Create a basic statistic table using the quantitative variables of the dataset.
quant <- c("age", "bmi", "children", "charges")

summary_stat <- psych::describe(data[quant], skew = FALSE) %>%
  round(2)

summary_stat %>%
  kable(
    caption = 'Basic statistics of quantitative variables',
    booktabs = TRUE,
    align = c('l', rep('c', 8))
  ) %>%
  kableExtra::kable_styling(
    bootstrap_options = c('striped', 'condensed'),
    font_size = 10,
    latex_options = "hold_position"
  ) 
```

The statistic table presents some of the basic statistic summaries for each quantitative variables. Including the number of records, mean, standard deviation, minimum, maximum, range, and standard error of the corresponding variable. Some interesting finding including the average of BMI, as the average BMI category of the dataset is classified as obesity, this suggested that most people probably are classified as overweight or more.

## Data Visualization
```{}
plot(data)
```
```{r}
esquisse::esquisser(data)
ggplot(data) +
  aes(x = sex, y = bmi) +
  geom_boxplot(fill = "#112446") +
  theme_minimal()
```
```{r}
esquisse::esquisser(data)
ggplot(data) +
  aes(x = age, y = smoker) +
  geom_boxplot(fill = "#112446") +
  theme_minimal()
```
```{r}
esquisse::esquisser(data)
ggplot(data) +
  aes(x = age, y = bmi) +
  geom_jitter(size = 0.5) +
  theme_minimal()
```
```{r}
esquisse::esquisser(data)
ggplot(data) +
  aes(x = age, y = children) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  theme_minimal()
```

# Model Building
```{r}
# Doing a 8/2 training, testing splits
set.seed(123)
train_ind <- sample(1:nrow(dummy_data), floor(0.8*nrow(dummy_data)))
set.seed(NULL)

train <- dummy_data[train_ind, ]
test <- dummy_data[-train_ind, ]
scaled_train <- scaled_data[train_ind, ]
scaled_test <- scaled_data[-train_ind, ]
```

## Multiple Linear Regression
```{r}
# Building the Multiple Linear Regression model
mlr_model <- lm(charges ~ ., data = scaled_train)

# Viewing the summary of the model
summary(mlr_model)

# Predicting with the test data
pred_mlr <- predict(mlr_model, newdata = scaled_test)

# Calculating Mean Squared Error (MSE) for the test set
MSE_mlr <- mean((scaled_test$charges - pred_mlr)^2)

# Displaying the MSE
print(paste("MSE for Multiple Linear Regression: ", MSE_mlr))
```

## LINE Assumption

The basic assumptions for a linear regression are: Linearity, Independence, Normality, and Equivariance. If any of these assumptions are violated, there are two concerns: incorrect conclusions and interpretations, or a lack of power for discovery.

We analyze these assumptions by using diagnostic plots on our full model.

```{r}
# Diagnostic plot - Residuals vs Fitted values
plot(mlr_model$fitted.values, mlr_model$residuals)
abline(h = 0, col = "red")

# Scatter plot with studentized deleted residuals
h <- hatvalues(mlr_model)
sum <- summary(mlr_model)
sig2hat <- sum$sigma^2
r <- mlr_model$residuals/sqrt(sig2hat*(1-h))
n <- 1338
p <- 7
t <- r*((n-p-1)/(n-p-r^2))^(1/2)
plot(h, t)

# Large leverage points
plot(h,xlab='Observation',ylab='Leverage',main='Leverage')
abline(h=2*p/n,lty=2,col="red")
abline(h=3*p/n,lty=2,col="blue")

# Cook's distance
D = (1/p)*r^2*h/(1-h)
plot(h,D)
plot(mlr_model, which=5)

# Diagnostic plot - QQ plot for residuals
qqnorm(mlr_model$residuals)
qqline(mlr_model$residuals, col = "red")

# Shapiro - Wilk Test for normality
shapiro.test(lm_fullModel$residuals)
```

Based on the Shapiro-Wilk test, the data is not normally distributed. However, due to the fact our dataset is quite large, we can reasonably ignore the issue. This is because (n - p) >= 30 where (1338 - 10) >= 30.

```{r}
# Check for multicollinearity using Variance Inflation Factor (VIF)
mlr_model_1 <- lm(charges ~ . -is_northeast, data = train)

# Using alias to check for the coefficients
print(alias(mlr_model_1))

# Calculate VIF
vif(mlr_model_1)
```

## LASSO Regression

```{r}
Xmat <- model.matrix(charges ~ . , data=scaled_data)[ ,-1]
y <- scaled_data$charges
```

```{r}
set.seed(123)
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)

X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
```

```{r}
set.seed (123)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train, 
                    alpha = 1, standardize = TRUE,
                    nfolds=10)
plot(cv.out)
```

```{r}
bestlam <- cv.out$lambda.min
pred_lasso <- predict(cv.out, s = bestlam,
                      newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
                     type = "coefficients")

RSS_lasso <- mean((y_test - pred_lasso)^2)
RSS_lasso
```

```{r}
coef_lasso
```

# Model Comparison

# Discussion

# Conclusion

# Code Appendix 
```{r}
#| ref.label = knitr::all_labels(),
#| echo = TRUE,
#| eval = FALSE

```