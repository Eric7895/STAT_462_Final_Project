---
title: "Applied Linear Regression on The Insurance Dataset"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
# Load necessary Libraries
library(tidyverse)
library(ggplot2)
library(janitor)
library(kableExtra)
library(glmnet)
library(psych)
library(faraway)

# Load Dataset 
data <- read.csv("insurance.csv")
```

# Group Members

Eric Wu, Derek Avery, Allison Schaedler, Xinyi Bao

# Background

The Insurance dataset extracted from Kaggle.com describes the medical costs for over 1300 individuals based on their age, sex, BMI in the United States. The goal of the project is to examine how many children, Smoker or not, and which region in the U.S they lived. The methods of the project are using multiple linear regression to find the best model with the lowest MSE value and use LASSO regression to make the comparison. In addition, there will be a detail diagnostic on the linear regression model for the assumptions, outliers, and other possible issues. 

The dataset come from https://www.kaggle.com/datasets/joebeachcapital/medical-insurance-costs/data

# Exploratory Data Analysis
```{r warning=FALSE, message=FALSE}
# Check for Null Value
na <- sum(is.na(data))

# Create dummy variable
dummy_data <- data %>%
  mutate(
    is_male = if_else(sex == "male", 1, 0),
    is_smoker = if_else(smoker == "yes", 1, 0),
    is_southwest = if_else(region == "southwest", 1, 0),
    is_southeast = if_else(region == "southeast", 1, 0),
    is_northwest = if_else(region == "northwest", 1, 0),
    is_northeast = if_else(region == "northeast", 1, 0)
  ) %>%
  select(-sex, -smoker, -region)

# Create standardized dataset for ML that requires scaling
xvars <- names(dummy_data)
scaled_data <- dummy_data
scaled_data[ , xvars] <- scale(scaled_data[ , xvars],
                            center = TRUE,
                            scale = TRUE)

scaled_data <- scale(dummy_data, center = TRUE, scale = TRUE)
scaleList <- list(scale = attr(scaled_data, "scaled:scale"),
                  center = attr(scaled_data, "scaled:center"))

scaled_data <- as.data.frame(scaled_data)
```

## Data Statistics
```{r}
# Create a basic statistic table using the quantitative variables of the dataset.
quant <- c("age", "bmi", "children", "charges")

summary_stat <- psych::describe(data[quant], skew = FALSE) %>%
  round(2)

summary_stat %>%
  kable(
    caption = 'Basic statistics of quantitative variables',
    booktabs = TRUE,
    align = c('l', rep('c', 8))
  ) %>%
  kableExtra::kable_styling(
    bootstrap_options = c('striped', 'condensed'),
    font_size = 10,
    latex_options = "hold_position"
  ) 
```

The statistic table presents some of the basic statistic summaries for each quantitative variables. Including the number of records, mean, standard deviation, minimum, maximum, range, and standard error of the corresponding variable. Some interesting finding including the average of BMI, as the average BMI category of the dataset is classified as obesity, this suggested that most people probably are classified as overweight or more.

## Data Visualization
```{r}
plot(data)
```

These plots display the relationships between each of the variables in the dataset.

```{r}
ggplot(data) +
  aes(x = sex, y = bmi) +
  geom_boxplot(fill = "#112446") +
  theme_minimal()
```

These boxplots display the relationship between the variables sex and BMI. There are a few outliers for each, but they are not significantly far from the rest of the data. The average male BMI is slightly higher than the average female BMI. 

```{r}
ggplot(data) +
  aes(x = age, y = smoker) +
  geom_boxplot(fill = "#112446") +
  theme_minimal()
```

These boxplots show the relationship between the variables age and smoker. The average age of smokers is slightly lower than the average age of non-smokers.There are no outliers visible on the plot.

```{r}
ggplot(data) +
  aes(x = age, y = bmi) +
  geom_jitter(size = 0.5) +
  theme_minimal()
```

This plot shows the distribution of the variables BMI and age. The data appear very random and evenly distributed across the plot. There is considerable variation in the datapoints.

```{r}
ggplot(data) +
  aes(x = age, y = children) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  theme_minimal()
```

This plot shows the relationship between age and amount of children. Most of the data is concentrated between 0 to 3 children across the whole spectrum of ages. 

# Model Building
```{r}
# Doing a 8/2 training, testing splits
set.seed(123)
train_ind <- sample(1:nrow(dummy_data), floor(0.8*nrow(dummy_data)))
set.seed(NULL)

train <- dummy_data[train_ind, ]
test <- dummy_data[-train_ind, ]
scaled_train <- scaled_data[train_ind, ]
scaled_test <- scaled_data[-train_ind, ]
```

## Multiple Linear Regression
```{r}
# Building the Multiple Linear Regression model
mlr_model <- lm(charges ~ ., data = scaled_train)

# Viewing the summary of the model
summary(mlr_model)

# Predicting with the test data
pred_mlr <- predict(mlr_model, newdata = scaled_test)

# Calculating Mean Squared Error (MSE) for the test set
actual <- scaled_test$charges * scaleList$scale["charges"] + scaleList$center["charges"]
pred <- pred_mlr * scaleList$scale["charges"] + scaleList$center["charges"]

MSE_mlr <- mean((actual - pred)^2)
RMSE_mlr <- round(sqrt(MSE_mlr), 2)

# Displaying the MSE
print(paste("RMSE for Multiple Linear Regression: ", RMSE_mlr))
```

***Model Summary:***

The model explains a significant amount of variance in the charges, with an R-squared value of 0.7533. This indicates that approximately 75.33% of the variability can be explained by the model.   
The Adjusted R-squared value is 0.7514, which suggests that the model fits the data well.    
The F-statistic is highly significant (p < 2.2e-16), indicating that the model as a whole is statistically significant.  

<br>

***Coefficients:***   

age, bmi, children, and is_smoker are significant predictors of charges, with p-values less than 0.05.
age has a coefficient of 0.282, suggesting that each additional year of age is associated with an increase in charges by a factor of 0.282, holding other variables constant.   
bmi is also a significant predictor, with each unit increase in BMI associated with an increase in charges by a factor of 0.178.
children has a positive coefficient (0.0557), indicating that having more children is associated with higher charges.   
is_smoker has the largest coefficient of 0.8058, which means being a smoker is associated with a substantial increase in charges. The variables is_male, is_southwest, is_southeast, and is_northwest are not statistically significant at the 0.05 level.    
However, is_southwest has a p-value close to the significance level, which may warrant further investigation.   
The is_northeast variable is not defined due to singularities, likely due to it being perfectly collinear with the other region variables.  

<br>

***Residual Analysis:***   

The Residuals vs Fitted plot shows a random scatter of residuals, which is good as it suggests that the variance of the residuals is constant (homoscedasticity). However, there seems to be a slight pattern with the residuals fanning out for larger fitted values, which could indicate potential issues with non-constant variance.   
The Normal Q-Q Plot shows that the residuals deviate from the line at the two ends, indicating that the residuals may not be normally distributed. This could affect the model's confidence intervals and hypothesis tests.  

<br>

***Other Analysis:***  

Key variables like age and smoking status are strong predictors of charges.   
The Mean Squared Error (MSE) for the model is 0.2265, which is relatively low and indicates good predictive performance.    

<br>

## LINE Assumption

The basic assumptions for a linear regression are: Linearity, Independence, Normality, and Equivariance. If any of these assumptions are violated, there are two concerns: incorrect conclusions and interpretations, or a lack of power for discovery.

We analyze these assumptions by using diagnostic plots on our full model.

```{r}
# Diagnostic plot - Residuals vs Fitted values
plot(mlr_model$fitted.values, mlr_model$residuals)
abline(lm(mlr_model$residuals ~ mlr_model$fitted.values), col="red")
```

By analyzing the residuals vs fitted values plot, it can be determined that there is relative equivariance between data points, given the line of best fit going through all these points has a slope of zero.

```{r}
# Large leverage points
h <- hatvalues(mlr_model)
sum <- summary(mlr_model)
p <- 12
n <- 1338
sig2hat <- sum$sigma^2
r <- mlr_model$residuals/sqrt(sig2hat*(1-h))
plot(h,xlab='Observation',ylab='Leverage',main='Leverage')
abline(h=2*p/n,lty=2,col="red")
abline(h=3*p/n,lty=2,col="blue")
```

By doing a test to see if we have any points with large leverage, we received results that indicate some points have large leverage in the model.

```{r}
# Cook's distance
D = (1/p)*r^2*h/(1-h)
plot(h,D)
plot(mlr_model, which=5)
```

Based on the results for the Cook's distance tests, there appears to be no data points that have a Cook's Distance greater than 1. Therefore, there are no outliers in the dataset. 

```{r}
# Diagnostic plot - QQ plot for residuals
qqnorm(mlr_model$residuals)
qqline(mlr_model$residuals, col = "red")
```

Based on the QQ plot for residuals, the majority of the data is along the line for normality, but there is a significant amount away from the line. This would suggest non-normality within the data, however using the central limit theorem and having 1338 observations we are able to ignore this on the plot and assume the data is normal.

```{r}
# Shapiro - Wilk Test for normality
shapiro.test(mlr_model$residuals)
```

Based on the Shapiro-Wilk test, the data is not normally distributed. However, due to the fact our dataset is quite large, we can reasonably use the central limit theorem. This is because (n - p) >= 30 where (1338 - 10) >= 30.

```{r}
# Calculate VIF
vif(lm(charges~., data = data))
```

Based on the result from the VIF function, our model is free from collinearity issues. 

## Adjusting the Model

```{r}
# Adjusting the model
adj_model <- lm(charges~age+bmi+children+is_smoker, data = scaled_train)
summary(adj_model)

# Predicting with the test data
pred_adj_model <- predict(adj_model, newdata = scaled_test)

# Calculating Mean Squared Error (MSE) for the test set
pred <- pred_adj_model * scaleList$scale["charges"] + scaleList$center["charges"]

MSE_mlr <- mean((actual - pred)^2)
RMSE_mlr <- round(sqrt(MSE_mlr), 2)

# Displaying the MSE
print(paste("RMSE for Multiple Linear Regression: ", RMSE_mlr))
```

This is the result of the adjusted model only using significant predictors compared to the full model where the estimates are not that different from the full model.

## LASSO Regression

We decided to implement another model so we can compare the adjusted model to a different model.

```{r}
Xmat <- model.matrix(charges ~ ., data=scaled_data)[ ,-1]
y <- scaled_data$charges
```

```{r}
set.seed(123)
train_ind <- sample(1:nrow(Xmat), floor(0.8*nrow(Xmat)))
set.seed(NULL)

X_mat_train <- Xmat[train_ind,]
X_mat_test <- Xmat[-train_ind,]
y_train <- y[train_ind]
y_test <- y[-train_ind]
```

```{r}
set.seed (123)
cv.out <- cv.glmnet(x= X_mat_train, y = y_train, 
                    alpha = 1, standardize = TRUE,
                    nfolds=10)
plot(cv.out)
```

Above is the graph demonstrate the relationship betten the MSE and the choice of lambda. Based on the graph, we will continue with the lambda that produce the least MSE.

```{r}
bestlam <- cv.out$lambda.min
pred_lasso <- predict(cv.out, s = bestlam,
                      newx = X_mat_test)
coef_lasso <- predict(cv.out, s = bestlam,
                     type = "coefficients")

#actual <- scaled_test$charges * scaleList$scale["charges"] + scaleList$center["charges"]
pred <- pred_lasso * scaleList$scale["charges"] + scaleList$center["charges"]

MSE_lasso <- mean((actual - pred)^2)
RMSE_lasso <- sqrt(MSE_lasso)
```
```{r}
coef_lasso
```
Above is the coefficient estimates for our lasso regression model, our lasso regression model utilized the entire dataset because one of the important properties of lasso regression is it's ability to do feature selections. Based on the coefficient estimates, we can tell that the variable "is_male", "is_southeast", and "is_northwest" shrinkage to zero as they are not important. The final RMSE value for our lasso regression is `r round(RMSE_lasso, 2)` which is lower than the RMSE of the adjusted model by `r round(RMSE_mlr - RMSE_lasso, 2)`. 

# Model Comparison

Despite having the ability to perform feature selection, the lasso regression model only outperforms the adjusted model by `r round(RMSE_mlr - RMSE_lasso, 2)`. We can only concluded that the lasso regression model, unlike our anticipation, is not the best approach for the insurance dataset. On the other hand, the adjusted model actually performed worse than the full model, indicating the possibility that the coefficient estimates of the insignificant variables might not be zero.  

# Discussion
Several data visualization techniques were utilized for exploratory analysis of the data. The boxplots of smokers stratified by age show that the average age of smokers is slightly lower than that of non-smokers. The relationship between the variables age and children were also plotted in a scatter plot. There is no visible pattern or correlation in the data. Most of the data is concentrated between 0 and 3 children across all ages. The points are random and evenly distributed. 

Further improvements could be made to the model by using AIC or BIC to complete forward or backward selection of certain predictors. However, using the most significant, we were able to obtain a sufficient model. Furthermore, there are anomalies in the data for the LINE assumptions, but since there are so many observations in our dataset, we are able to ignore them by CLT, but they should be taken into account later. In addition, we can use the anova table in the future to examine whether the insignificant variables actually have a coefficient estimates that's closed to zero. 

# Conclusion

In conclusion, this project demonstrated a completely data analysis pipeline, from data collection to model comparison. In addition, we practice the concepts regarding different diagnostic graphs, and examine the lasso regression model in a meaningful way. Based on the results, we concluded that the full model is probably matches all the assumptions of a linear regression model, except that the full model contains some large leverage points. And the lasso regression model has shown a slightly improvement in predicting the actual insurance cost.  

# Code Appendix 
```{r}
#| ref.label = knitr::all_labels(),
#| echo = TRUE,
#| eval = FALSE

```